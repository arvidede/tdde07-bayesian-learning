---
title: "TDDE07 - Lab 2"
author:
- "Arvid Edenheim - arved490"
- "Sophie Lindberg - sopli268"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
  \usepackage{float}
  \floatplacement{figure}{H}
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

# 1. Linear and polynomial regression

## 1a)

Before looking at the data, we came to the conclusion that the intercept should be about -10 degrees, with a cyclic temperature throughout the year and a maximum at 7 months. The variance was adjusted in order for the prior regression curve to be in accordance with our prior beliefs. Draws were made from the prior and plotted along with the temperature readings for comparison, as can be seen in Figure 1.

$\mu_{0}$ = [-10, 100, -90]

$v_0 = 4\\$
$\sigma_0^{2} = 8\\$

$\Omega_{0}$ = $$
\left(\begin{array}{ccc} 
10 & 0 & 0\\
0 & 10 & 0\\
0 & 0 & 10\\
\end{array}\right)
$$


![Plotting the chosen hyperparameters](plots/lab2_1_a)

\newpage

## 1b)

![Marginal posterior of the beta parameters](plots/lab2_1_b_hist)

\newpage

The figure below shows the plotted data along with the curve for the posterior median of the regression function. Furthermore, a 95 % credible interval has been calculated. The interval bands doesn't contain most od the data points due to the high credibility.

![Posterior credible intervals](plots/lab2_1_B)

\newpage

## 1c)

The highest expected temperature was calculated using the derivative of the time function, with respect to time.

$$\begin{aligned}
\frac{\partial f}{\partial t} = \beta_1 + 2\beta_2 t = 0 \iff t = -\frac{\beta_1}{2\beta_2}\\
0\leq t\leq 1 \Rightarrow 0\leq \bar{x} \leq 366 \iff \bar{x} = 366 * t\\
\end{aligned}$$

The histogram of $\bar{x}$ can be seen in Figure 4.

![$\bar{x}$](plots/lab2_1_c.png)

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
  \hline
 & Min. & 1st Qu. & Median. & Mean & 3rd Qu. & Max. \\ 
  \hline
1 & 194.54 & 198.30 & 199.27 & 199.29 & 200.26 & 203.37 \\ 
   \hline
\end{tabular}
\caption{Summary of $\bar{x}$} 
\end{table}

This is in line with the output from `which.max(y_med)` = 200, where `y_med` is the posterior mean from a).

\newpage

## 1d)

The new beta parameters would be set to zeror due to suspicion that the introduced variables might not be needed.
Regarding the covariance matrix, we want low variance (high bias) to avoid overfitting => High diagonal values in $\Omega_0$ for the new variables, with the rest set to zero since we can't say anything about the covariance between the new variables.


\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrrrr}
  \hline
 $\beta_0$. & $\beta_1$ & $\beta_2$ & $\beta_3$ & $\beta_4$ & $\beta_5$ & $\beta_6$ & $\beta_7$\\ 
  \hline
-11.53113 & 98.43113 & -90.38911 & 0 & 0 & 0 & 0 & 0\\ 
   \hline
\end{tabular}
\caption{$\mu_0$} 
\end{table}

$\Omega_0$ = $$
\begin{pmatrix}
376 & 183.5 & 122.5 & 0 & 0 & 0 & 0 & 0 \\
183.5 & 132.5 & 92.5 & 0 & 0 & 0 & 0 & 0 \\
122.5 & 92.5 & 83.7 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 100 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 100 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 100 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 100 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 100 \\
\end{pmatrix}
$$

\newpage

# 1 - Code

```{r, echo=TRUE, eval=FALSE}

library(mvtnorm)
library(invgamma)
data = read.table("TempLinkoping.txt", header=TRUE)

######## A ########
I <- diag(3)
mu <- c(-10, 100, -90)
omega <- 10 * I
v <- 4
s2 <- 8
x <- seq(0, 10, by=0.01) 


# Inverse chisquare from lab1  
draw_sigma <- function(v, s2) {
  X_draw <- rchisq(1, v)
  return (v * s2 / X_draw)
}

# Given temperature regression function
temp <- function(beta, time) {
  return (beta[1] + beta[2] * time + beta[3] * (time ^ 2)) + rnorm(0,1)
}

par(mfrow=c(3,3))

plot(data$time, data$temp)

for (i in 1:8) {
  sigma_sq = draw_sigma(v, s2)
  beta = rmvnorm(1, mu, sigma_sq*solve(omega))
  y = temp(beta[1,], data$time)
  plot(data$time, y)
}

dev.off()


######## B ########
n_draws <- 1000
ones <- c(rep(1, length(data$time)))
x2 <- data$time^2
X <- cbind(ones, data$time, x2)
beta_hat <- solve(t(X)%*%X)%*%t(X)%*%data$temp
omega_n <- t(X) %*% X + omega
mu_n <- solve(omega_n) %*% (t(X) %*% X %*% beta_hat + omega %*% mu)

v_n <- v + length(data$time)
s2_n <- (v*s2 + (t(data$temp)%*%data$temp + t(mu)%*%omega%*%mu - t(mu_n)%*%omega_n%*%mu_n))/v_n

sigma_post = draw_sigma(v_n, s2_n)
beta_post = rmvnorm(n_draws, mu_n, solve(omega_n)*sigma_post[1])

Y = matrix(nrow=n_draws, ncol=366)
for (i in 1:n_draws) {
  Y[i,] = temp(beta_post[i,], data$time)
}

y_med = c()
y_up = c()
y_low = c()

for (i in 1:366) {
  y_med = c(y_med, median(Y[,i]))
  y_low = c(y_low, quantile(Y[,i], 0.025))
  y_up = c(y_up, quantile(Y[,i], 0.975))
}

par(mfrow=c(1,3))
hist(beta_post[,1], freq = FALSE, xlab = "Beta 0", main='')
hist(beta_post[,2], freq = FALSE, xlab = "Beta 1", main='')
hist(beta_post[,3], freq = FALSE, xlab = "Beta 2", main='')
dev.off()

plot(data$temp, type='p', col='lightgray')
lines(y_med, type='l')
lines(y_low, type='l', col='red')
lines(y_up, type='l', col='blue')

legend("bottomright", 
       legend = c("Mean","Lower", "Upper"),
       fill = c("black", "blue", "red"))

######## C ########

x_max = - 366 * beta_post[,2]/(2 * beta_post[,3])
hist(x_max, main='', xlab='Time')
dev.off()

######## D ########

# Prior
# Mu_0 = [...mu, 0, 0, 0, 0] due to suspicion that the introduced variables might not be needed
# Omega_0, want low variance (high bias to avoid overfitting) => High diagonal values in omega_0for the new variables, with the rest set to zero since we can't say anything about the covariance between the new variables

new_mu = c(mu_n, rep(0, 5))
new_omega <- 100 * diag(8)
new_omega[1:3,1:3] = omega_n

```

\newpage
# 2.Posterior approximation for classification with logistic regression

## 2b) 

Numerical values for beta: 

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrrrr}
  \hline
 $\beta_0$. & $\beta_1$ & $\beta_2$ & $\beta_3$ & $\beta_4$ & $\beta_5$ & $\beta_6$ & $\beta_7$\\ 
  \hline
0.62672884 & 0.01979113 & 0.18021897 & 0.16756670 & -0.14459669 & -0.08206561 & -1.35913317 & -0.02468351\\ 
   \hline
\end{tabular}
\caption{$\beta$} 
\end{table}

Numerical values for the hessian: 

Hessian = $$
\begin{pmatrix}
2.266022568 & 3.338861e-03 & -6.545121e-02 & -1.179140e-02 &  0.0457807243 & -3.029345e-02 & -0.1887483542 & -0.0980239285 \\
0.003338861 & 2.528045e-04 & -5.610225e-04 & -3.125413e-05 & 0.0001414915 & -3.588562e-05 & 0.0005066847 & -0.0001444223\\
-0.065451206 & -5.610225e-04 &  6.218199e-03 & -3.558209e-04 & 0.0018962893 &-3.240448e-06 & -0.0061345645 & 0.0017527317\\
-0.011791404 & -3.125413e-05 & -3.558209e-04 & 4.351716e-03 & -0.0142490853 & -1.340888e-04 & -0.0014689508 & 0.0005437105 \\
 0.045780724 & 1.414915e-04 & 1.896289e-03 & -1.424909e-02 & 0.0555786706 & -3.299398e-04 & 0.0032082535 & 0.0005120144 \\
-0.030293450 & -3.588562e-05 & -3.240448e-06 & -1.340888e-04 & -0.0003299398 & 7.184611e-04 & 0.0051841611  & 0.0010952903 \\
-0.188748354 & 5.066847e-04 & -6.134564e-03 & -1.468951e-03 & 0.0032082535 & 5.184161e-03 & 0.1512621814 & 0.0067688739\\
-0.098023929 & -1.444223e-04 & 1.752732e-03 & 5.437105e-04 &0.0005120144 & 1.095290e-03 & 0.0067688739 & 0.0199722657\\
\end{pmatrix}
$$

![Plotting the chosen hyperparameters](plots/lab2_2_b)

According to the figure, this parameter is very close to zero and thus, this variable is not of importance. 

# 2 - Code

